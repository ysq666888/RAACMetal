{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6e26e6",
   "metadata": {},
   "source": [
    "* cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bccf52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = f\"./results/1/2\"\n",
    "print(a.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "953cb780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 root_path: /data1/ysq/GPU文件/需备份文件/T5模型参数\n",
      "temp:data_make/zn/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/zn/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/co/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/co/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/cu/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/cu/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/fe/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/fe/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/fe2/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/fe2/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/k/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/k/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/mg/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/mg/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/mn/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/mn/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/na/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/na/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/ni/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/ni/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/zn/train.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n",
      "temp:data_make/zn/test.fasta\n",
      "cd-hit运行成功!\n",
      "输出文件：\n",
      "  - 代表序列: clustered_output\n",
      "  - 聚类信息: clustered_output.clstr\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "root_path = os.getcwd()\n",
    "print(\"当前 root_path:\", root_path)\n",
    "\n",
    "def run_cd_hit(input_fasta, output_prefix, similarity=0.9, threads=4):\n",
    "    \"\"\"\n",
    "    output_prefix: 输出文件前缀\n",
    "    similarity: 相似性阈值 (0-1)\n",
    "    threads: 使用的CPU线程数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 构建cd-hit命令\n",
    "    cmd = [\n",
    "        'cd-hit',\n",
    "        '-i', input_fasta,\n",
    "        '-o', output_prefix,\n",
    "        '-c', str(similarity),\n",
    "        '-T', str(threads),\n",
    "        '-M', '16000'  # 内存限制16GB\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # 执行命令\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        print(\"cd-hit运行成功!\")\n",
    "        print(\"输出文件：\")\n",
    "        print(f\"  - 代表序列: {output_prefix}\")\n",
    "        print(f\"  - 聚类信息: {output_prefix}.clstr\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"cd-hit运行失败: {e}\")\n",
    "        print(f\"错误输出: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def get_temp_fasta(input_file:str):\n",
    "    temp_file: str = input_file.replace('.txt', '.fasta')\n",
    "    print(f'temp:{temp_file}')\n",
    "    store_site: dict = dict()\n",
    "    with open(temp_file, 'w', encoding='utf-8') as writer_file:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    protein_name = line\n",
    "                    seq = next(f)\n",
    "                    sites = next(f)\n",
    "                    store_site[protein_name] = sites\n",
    "                    writer_file.writelines([protein_name, seq])\n",
    "    run_cd_hit(temp_file, \"clustered_output\", 0.9, 8)\n",
    "    final_file: str = input_file.replace('.txt', '_final.txt')\n",
    "    with open(final_file, 'w', encoding='utf-8') as f:\n",
    "        with open(temp_file, 'r', encoding='utf-8') as reader_file:\n",
    "            for line in reader_file:\n",
    "                protein_name = line\n",
    "                seq = next(reader_file)\n",
    "                sites = store_site[protein_name]\n",
    "                f.writelines([protein_name, seq, sites])\n",
    "    os.remove(temp_file)\n",
    "    os.remove(os.path.join(root_path, 'clustered_output'))\n",
    "    os.remove(os.path.join(root_path, 'clustered_output.clstr'))\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    ions = ['zn', 'co', 'cu', 'fe', 'fe2', 'k', 'mg', 'mn', 'na', 'ni', 'zn']\n",
    "    types = ['train', 'test']\n",
    "    for ion in ions:\n",
    "        for train in types:\n",
    "            input_dir = f'data_make/{ion}'\n",
    "            file_path = os.path.join(input_dir, f'{train}.txt')\n",
    "            get_temp_fasta(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b8eeb",
   "metadata": {},
   "source": [
    "* 滑动窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0893ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 root_path: /data1/ysq/GPU文件/需备份文件/T5模型参数\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import os\n",
    "root_path = os.getcwd()\n",
    "print(\"当前 root_path:\", root_path)\n",
    "\n",
    "def slice_fragment(\n",
    "    input_file: str, \n",
    "    output_dir: str = '.', \n",
    "    output_file_profix: str = '',\n",
    "    focus: Union[list, bool]=None, \n",
    "    return_output:bool=False,\n",
    "    pad_char='-',\n",
    "    window_size=25\n",
    "    ) -> Union[None, tuple]:\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "        slice length==25 fragemnt of protein sequence.\n",
    "        the postive fragment is the central residue is bind site and focus\n",
    "    :param input_file:\n",
    "    :param focus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    if not focus:\n",
    "        focus = [\"C\", \"D\", \"E\", \"G\", \"H\", \"K\", \"N\", \"R\", \"S\"]\n",
    "        pass\n",
    "    positive_fragments: str = ''\n",
    "    negative_fragments: str = ''\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                name: str = line.strip().split(\"\\t\")[0]\n",
    "                seq: str = next(f).strip()\n",
    "                site: str = next(f).strip()\n",
    "                padded_sequence = pad_char * half_window + seq + pad_char * half_window\n",
    "                fragments = [padded_sequence[i:i+window_size] for i in range((len(site)))]\n",
    "                positive_fragment = [name+'\\n'+fragments[i]+'\\n'+'0'*half_window+site[i]+'0'*half_window+'\\n' for i in range(len(site)) if site[i] == '1']\n",
    "                negative_fragment = [name+'\\n'+fragments[i]+'\\n'+'0'*half_window+site[i]+'0'*half_window+'\\n' for i in range(len(site)) if site[i] == '0']\n",
    "                positive_fragments += ''.join(positive_fragment)\n",
    "                negative_fragments += ''.join(negative_fragment)\n",
    "\n",
    "\n",
    "    if return_output:\t\t\t\t\t\t\t\n",
    "        return positive_fragments, negative_fragments\n",
    "    else:\n",
    "        with open(f'{output_dir}/{output_file_profix}positive_fragment.txt', 'w', encoding='utf-8') as writer:\n",
    "            writer.write(positive_fragments)\n",
    "        with open(f'{output_dir}/{output_file_profix}negative_fragment.txt', 'w', encoding='utf-8') as writer:\n",
    "            writer.write(negative_fragments)\n",
    "\n",
    "\n",
    "ions = ['zn', 'co', 'cu', 'fe', 'fe2', 'k', 'mg', 'mn', 'na', 'ni', 'zn']\n",
    "types = ['train', 'test']\n",
    "for ion in ions:\n",
    "    for train in types:\n",
    "        save_dir = os.path.join(root_path, 'data_make', 'noreduce', train, f'{ion}_{train}')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_path = os.path.join(root_path, 'data_make', ion, f'{train}_final.txt')\n",
    "        slice_fragment(\n",
    "            input_file=file_path, \n",
    "            output_dir=save_dir,\n",
    "            output_file_profix=''\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747cbf96",
   "metadata": {},
   "source": [
    "* 约化氨基酸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbcb67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 root_path: /data1/ysq/GPU文件/需备份文件/T5模型参数\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DSSP running', ncols=80: 100%|██████████| 671/671 [03:42<00:00,  3.01it/s]\n",
      "DSSP running', ncols=80: 100%|██████████| 671/671 [00:26<00:00, 24.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "cpu_count = os.cpu_count()\n",
    "root_path = os.getcwd()\n",
    "print(\"当前 root_path:\", root_path)\n",
    "\n",
    "class IrapSeq:\n",
    "    def __init__(self) -> None:\n",
    "        self.irap_path: str = os.path.join(root_path, 'data_make', 'irap.txt')\n",
    "        self.iraps: dict = self.__readirap__()\n",
    "    \n",
    "    def __readirap__(self) -> dict[str, str]:\n",
    "        irap: dict[str, str] = dict()\n",
    "        with open(self.irap_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split(' ')\n",
    "                the_type: str = line[1]\n",
    "                the_size: str = line[3]\n",
    "                context: str = line[-1]\n",
    "                name = f'type_{the_type}+size_{the_size}+{context}'\n",
    "                assert name not in irap.keys(), f'{name} 重复'\n",
    "                irap[name] = context\n",
    "        return irap\n",
    "        \n",
    "    def irap_dict(self, type:str, size: str) -> str:\n",
    "        name: str = f'type:{type}+size:{size}'\n",
    "        return self.iraps[name]\n",
    "    \n",
    "    def irap_dicts(self) -> dict[str, str]:\n",
    "        return self.iraps\n",
    "        \n",
    "    def irap(self, seq:str, type_and_size: Union[bool, str] = None) -> str:\n",
    "        if type_and_size:\n",
    "            name: str = type_and_size\n",
    "        else:\n",
    "            name: str = f'type:0+size:1'\n",
    "        irap_context: list = self.iraps[name].split(\"-\")\n",
    "        return self.__seqtoirap__(seq.upper(), irap_context)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __seqtoirap__(seq: str, irap_list: list) -> str:\n",
    "        irap_seq: str = ''\n",
    "        for res in seq:\n",
    "            if res == '-':  \n",
    "                irap_seq += '-'\n",
    "            else:\n",
    "                matched = False\n",
    "                for irap_type in irap_list:\n",
    "                    if res in irap_type:\n",
    "                        irap_seq += irap_type[0]\n",
    "                        matched = True\n",
    "                        break   # 一旦匹配到就退出，避免重复追加\n",
    "                if not matched:\n",
    "                    irap_seq += res   # 没匹配到时给默认符号\n",
    "        return irap_seq\n",
    "\n",
    "\n",
    "def process_one(input_dir, save_dir, irap, k):\n",
    "    with open(f'{input_dir}/positive_fragment.txt', 'r') as positive:\n",
    "        output_path = f'{save_dir}/positive_fragment.txt'\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            pass\n",
    "        with open(output_path, 'w') as writer:\n",
    "            for line in positive:\n",
    "                if line.startswith('>'):\n",
    "                    protein_name = line.strip()\n",
    "                    seq = next(positive).strip()\n",
    "                    sites = next(positive).strip()\n",
    "                    output = irap.irap(seq=seq, type_and_size=k)\n",
    "                    assert len(output) == 25, f'len is large, {len(seq)}, {len(output)}, {seq}, {output}, {k}'\n",
    "                    writer.write('\\n'.join([protein_name, output, sites])+'\\n')\n",
    "\n",
    "    with open(f'{input_dir}/negative_fragment.txt', 'r', encoding='utf-8') as negative:\n",
    "        output_path = f'{save_dir}/negative_fragment.txt'\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            pass\n",
    "        with open(output_path, 'w') as writer:\n",
    "            for line in negative:\n",
    "                if line.startswith('>'):\n",
    "                    protein_name = line.strip()\n",
    "                    seq = next(negative).strip()\n",
    "                    sites = next(negative).strip()\n",
    "                    output = irap.irap(seq=seq, type_and_size=k)\n",
    "                    assert len(output) == 25, f'len is large, {seq}, {output}'\n",
    "                    writer.write('\\n'.join([protein_name, output, sites])+'\\n')\n",
    "\n",
    "\n",
    "def select_dataset():\n",
    "    \n",
    "    irap = IrapSeq()\n",
    "    types = irap.irap_dicts()\n",
    "    # ions = ['zn', 'co', 'cu', 'fe', 'fe2', 'k', 'mg', 'mn', 'na', 'ni', 'zn']\n",
    "    ions = ['ca']\n",
    "    train_test = ['train', 'test']\n",
    "    for ion in ions:\n",
    "        for train in train_test:\n",
    "            with ProcessPoolExecutor(max_workers=cpu_count-4) as executor:\n",
    "                futures = []\n",
    "                for k, v in types.items():\n",
    "                    save_dir = '/'.join(\n",
    "                        [\n",
    "                        root_path, \n",
    "                        'data_make',\n",
    "                        'reduce', \n",
    "                        train, \n",
    "                        f'{ion}_{train}', \n",
    "                        k]\n",
    "                    )\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    input_dir = os.path.join(root_path, 'data_make', 'noreduce', train, f'{ion}_{train}')\n",
    "                    futures.append(executor.submit(process_one, input_dir, save_dir, irap, k))\n",
    "                for f in tqdm(as_completed(futures), total=len(futures), desc=\"DSSP running', ncols=80\"):\n",
    "                    try:\n",
    "                        f.result()\n",
    "                    except Exception as e:\n",
    "                        print(f\"error: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    select_dataset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d6091",
   "metadata": {},
   "source": [
    "* 转化为tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01f82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 root_path: /data1/ysq/GPU文件/需备份文件/T5模型参数\n",
      "type_49+size_17+C-FY-W-ML-IV-G-P-A-T-S-N-H-Q-E-D-R-K\n",
      "正样本数量: 2407\n",
      "目标负样本数量: 3611\n",
      "负样本总数: 93274\n",
      "- 正样本: 2407\n",
      "- 负样本: 3611\n",
      "/data1/ysq/GPU文件/需备份文件/T5模型参数/data_make/reduce/train/k_train/type_49+size_17+C-FY-W-ML-IV-G-P-A-T-S-N-H-Q-E-D-R-K/train_4.txt\n",
      "type_49+size_17+C-FY-W-ML-IV-G-P-A-T-S-N-H-Q-E-D-R-K\n",
      "正样本数量: 293\n",
      "目标负样本数量: 440\n",
      "负样本总数: 10398\n",
      "- 正样本: 293\n",
      "- 负样本: 440\n",
      "/data1/ysq/GPU文件/需备份文件/T5模型参数/data_make/reduce/test/k_test/type_49+size_17+C-FY-W-ML-IV-G-P-A-T-S-N-H-Q-E-D-R-K/test_4.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from typing import Union\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from  math import ceil\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from T5Model.Configs import Config\n",
    "from data_make.utils import ion_reduce_types\n",
    "root_path = os.getcwd()\n",
    "print(\"当前 root_path:\", root_path)\n",
    "\n",
    "\n",
    "class FullTokenizer(object):\n",
    "\n",
    "\n",
    "\tdef __init__(self, vocab_file: str) -> None:\n",
    "\t\tself.vocab: dict[str, int] = dict()\n",
    "\t\twith open(vocab_file, 'r') as vocab_file:\n",
    "\t\t\tvocab_lines: list[str] = vocab_file.readlines()\n",
    "\t\t\tindex: int = 0\n",
    "\t\t\tfor vocab_line in vocab_lines:\n",
    "\t\t\t\ttoken: str = vocab_line.strip()\n",
    "\t\t\t\tself.vocab[token] = index\n",
    "\t\t\t\tindex += 1\n",
    "\n",
    "\tdef get_vocab(self) -> dict[str, int]:\n",
    "\t\treturn self.vocab\n",
    "\n",
    "\n",
    "def loading_data(input_file: str) -> tuple[list, list, list]:\n",
    "\n",
    "\tprint(input_file)\n",
    "\tnames: list = []\n",
    "\tseqs: list = []\n",
    "\tlabels: list = []\n",
    "\twith open(input_file, 'r') as f:\n",
    "\t\tlines = iter(f.readlines())\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif line.startswith('>'):\n",
    "\t\t\t\tname: str = line.strip().split(' ')[0]\n",
    "\n",
    "\t\t\t\tseq: str = next(lines).strip()\n",
    "\n",
    "\t\t\t\tlabel: str = next(lines).strip()\n",
    "\t\t\t\tlabel: list = list(map(int, label))\n",
    "\n",
    "\t\t\t\tnames.append(name)\n",
    "\t\t\t\tseqs.append(seq)\n",
    "\t\t\t\tlabels.append(label)\n",
    "\treturn names, seqs, labels\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(tokens: list[str], vocab: dict[str, int]) -> list[int]:\n",
    "\n",
    "\tids: list = []\n",
    "\tfor position in range(len(tokens)):\n",
    "\t\tif tokens[position] in vocab.keys():\n",
    "\t\t\tids += [vocab[tokens[position].upper()]]\n",
    "\t\telse:\n",
    "\t\t\tids += [vocab['-']]\n",
    "\treturn ids\n",
    "\n",
    "\n",
    "def get_features(\n",
    "\t\tname: str,\n",
    "\t\tseq: str,\n",
    "\t\tlabel: Union[list, int],\n",
    "\t\tvocab: dict[str, int],\n",
    "\t\tmax_length: int) -> tuple[bytes, list, list, list]:\n",
    "\n",
    "\tpdb_id: bytes = name.encode('utf-8')\n",
    "\tmax_length: int = max_length - 2\n",
    "\tassert len(seq) == max_length, f'seq length:{len(seq)} < max_length {max_length}'\n",
    "\n",
    "\tseq_list = [x if x != '-' else '[PAD]' for x in seq]\n",
    "\n",
    "\tstart: int = vocab[\"[CLS]\"]\n",
    "\tend: int = vocab[\"[SEP]\"]\n",
    "\n",
    "\tseq_list: list[int] = convert_tokens_to_ids(seq_list, vocab)\n",
    "\n",
    "\tseq_list: list[int] = [start] + seq_list + [end]\n",
    "\n",
    "\tpad: list[int] = [1] + [1 if x != '-' else 0 for x in seq] + [1]\n",
    "\n",
    "\tlabels: list[int] = [0] + label + [0]\n",
    "\t# print(f'pdb_id:{pdb_id}\\nseq_id:{len(seq_id)}\\npad:{len(pad)}\\nlabel:{len(labels)}')\n",
    "\treturn pdb_id, seq_list, pad, labels\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "\t\tfile_path: str,\n",
    "\t\toutput_path: str,\n",
    "\t\tvocab_file: str,\n",
    "\t\t) -> None:\n",
    "\n",
    "\tconfig: Config = Config()\n",
    "\tnames, seqs, labels = loading_data(file_path)\n",
    "\tvocabs: dict[str, int] = FullTokenizer(vocab_file).get_vocab()\n",
    "\twith tf.io.TFRecordWriter(output_path) as writer:\n",
    "\t\tfor line_index in range(len(names)):\n",
    "\t\t\tpdb_id, seq_id, pad, label = get_features(\n",
    "\t\t\t\tnames[line_index],\n",
    "\t\t\t\tseqs[line_index],\n",
    "\t\t\t\tlabels[line_index],\n",
    "\t\t\t\tvocabs,\n",
    "\t\t\t\tconfig.max_seq_length)\n",
    "\n",
    "\t\t\tfeatures: dict[str, tf.Tensor] = {\n",
    "\t\t\t\t'protein_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[pdb_id])),\n",
    "\t\t\t\t'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=seq_id)),\n",
    "\t\t\t\t'input_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=pad)),\n",
    "\t\t\t\t'labels': tf.train.Feature(int64_list=tf.train.Int64List(value=label)),\n",
    "\t\t\t}\n",
    "\t\t\texample = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\t\t\twriter.write(example.SerializeToString())\n",
    "\t\twriter.close()\n",
    "\n",
    "\n",
    "\n",
    "def add_negative(\n",
    "\tpositive_file: str,\n",
    "\tnegative_file: str, \n",
    "\toutput_file: str, \n",
    "\tscale=False, \n",
    "\trandom_seed=42) -> None:\n",
    "\n",
    "\tif scale:\n",
    "\t\trandom.seed(random_seed)\n",
    "\n",
    "\t\twith open(positive_file, 'r') as pos_f:\n",
    "\t\t\tpositive_count = sum(1 for line in pos_f if line.startswith('>'))\n",
    "\t\tprint(f\"正样本数量: {positive_count}\")\n",
    "\t\t\n",
    "\t\ttarget_negative_count = ceil(positive_count * (10-scale) / scale)\n",
    "\t\tprint(f\"目标负样本数量: {target_negative_count}\")\n",
    "\t\t\n",
    "\t\t# 统计负样本总数\n",
    "\t\tnegative_total = 0\n",
    "\t\twith open(negative_file, 'r') as neg_f:\n",
    "\t\t\tnegative_total = sum(1 for line in neg_f if line.startswith('>'))\n",
    "\t\tprint(f\"负样本总数: {negative_total}\")\n",
    "\t\t\n",
    "\t\tactual_negative = min(target_negative_count, negative_total)\n",
    "\t\t\n",
    "\t\tif negative_total <= actual_negative:\n",
    "\t\t\tselected_indices = list(range(negative_total))\n",
    "\t\telse:\n",
    "\t\t\tselected_indices = random.sample(range(negative_total), actual_negative)\n",
    "\t\t\n",
    "\t\tselected_line_ranges = []\n",
    "\t\tfor idx in selected_indices:\n",
    "\t\t\tstart_line = idx * 3\n",
    "\t\t\tselected_line_ranges.append(range(start_line, start_line + 3))\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "\t\twith open(positive_file, 'r') as positive:\n",
    "\t\t\tpositive_content = positive.read()\n",
    "\t\t\n",
    "\t\twith open(output_file, 'w') as output:\n",
    "\t\t\toutput.write(positive_content)\n",
    "\t\t\t\n",
    "\t\t\twith open(negative_file, 'r') as negative:\n",
    "\t\t\t\tnegative_lines = negative.readlines()\n",
    "\t\t\t\n",
    "\t\t\tfor line_range in selected_line_ranges:\n",
    "\t\t\t\tfor i in line_range:\n",
    "\t\t\t\t\tif i < len(negative_lines):\n",
    "\t\t\t\t\t\toutput.write(negative_lines[i])\n",
    "\t\t\n",
    "\t\tprint(f\"- 正样本: {positive_count}\")\n",
    "\t\tprint(f\"- 负样本: {actual_negative}\")\n",
    "\telse:\n",
    "\t\twith open(output_file, 'w') as output:\n",
    "\t\t\twith open(positive_file, 'r') as positive:\n",
    "\t\t\t\tfor line in positive:\n",
    "\t\t\t\t\toutput.write(line)\n",
    "\t\t\twith open(negative_file, 'r') as negative:\n",
    "\t\t\t\tfor line in negative:\n",
    "\n",
    "\t\t\t\t\toutput.write(line)\n",
    "\t\t\t\t\t\n",
    "\n",
    "def reduce() -> None:\n",
    "\t\"\"\"convert txt or fasta to tfrecord\"\"\"\n",
    "\ttrain_test: str = ['train', 'test']\n",
    "\tvocab_path = os.path.join(root_path, 'data_make', 'vocab.txt')\n",
    "\tscale = 4\n",
    "\tfor train in train_test:\n",
    "\t\ttarget_dir = os.path.join(root_path, 'data_make', 'reduce', train)\n",
    "\t\ttargets: list[str] = os.listdir(target_dir)\n",
    "\t\tfor target in targets:\n",
    "\t\t\tif 'k' not in target:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tpath_dir: str = f'{target_dir}/{target}'\n",
    "\t\t\tfiles: list = os.listdir(path_dir)\n",
    "\t\t\tfor file_dir in files:\n",
    "\t\t\t\tif file_dir != ion_reduce_types[target.split('_')[0]]:\n",
    "\t\t\t\t# if file_dir != 'type_0+size_20+A-C-D-E-F-G-H-I-K-L-M-N-P-Q-R-S-T-V-W-Y':\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tprint(file_dir)\n",
    "\t\t\t\tfile_path: str = f'{path_dir}/{file_dir}'\n",
    "\t\t\t\tpositive_file: str = f'{file_path}/positive_fragment.txt'\n",
    "\t\t\t\tnegative_file: str = f'{file_path}/negative_fragment.txt'\n",
    "\t\t\t\toutput_file = f'{file_path}/{train}_{scale}.txt' if scale else f'{file_path}/{train}.txt'\n",
    "\t\t\t\ttf_file = output_file.replace('txt', 'tfrecord') \n",
    "\t\t\t\tif os.path.exists(output_file):\n",
    "\t\t\t\t\tadd_negative(positive_file, negative_file, output_file=output_file, scale=scale)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tadd_negative(positive_file, negative_file, output_file=output_file, scale=scale)\n",
    "\t\t\t\tif os.path.exists(f'{file_path}/{train}.tfrecord'):\n",
    "\t\t\t\t\ttokenize(file_path=output_file, output_path=tf_file, vocab_file=vocab_path)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttokenize(file_path=output_file, output_path=tf_file, vocab_file=vocab_path)\n",
    "\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "\t\"\"\"convert txt or fasta to tfrecord\"\"\"\n",
    "\ttrain: str = 'train'\n",
    "\toperation: str = 'create'\n",
    "\ttarget_dir: str = f'./noreduce/{train}'\n",
    "\t# os.makedirs(target_dir, exist_ok=True)\n",
    "\ttargets: list[str] = os.listdir(target_dir)\n",
    "\tfor target in targets:\n",
    "\t\tpath_dir: str = f'{target_dir}/{target}'\n",
    "\t\tprint(path_dir)\n",
    "\t\tif operation == 'create':\n",
    "\t\t\tpositive_file: str = f'{path_dir}/positive_fragment.txt'\n",
    "\t\t\tnegative_file: str = f'{path_dir}/negative_fragment.txt'\n",
    "\t\t\tinput_path: str = f'{path_dir}/{train}.txt'\n",
    "\t\t\t\n",
    "\t\t\tif os.path.exists(input_path):\n",
    "\t\t\t\tadd_negative(positive_file, negative_file, train=train, output_dir=path_dir)\n",
    "\t\t\telse:\n",
    "\t\t\t\tadd_negative(positive_file, negative_file, train=train, output_dir=path_dir)\n",
    "\t\t\tif os.path.exists(f'{path_dir}/{train}.tfrecord'):\n",
    "\t\t\t\ttokenize(file_path=input_path, output_path=path_dir, vocab_file='./vocab.txt', train=train)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttokenize(file_path=input_path, output_path=path_dir, vocab_file='./vocab.txt', train=train)\n",
    "\n",
    "\t\t\t\n",
    "if __name__ == '__main__':\n",
    "\t# main()\n",
    "\treduce()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c2119",
   "metadata": {},
   "source": [
    "* 结果统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "root_dir: str = '..'\n",
    "target: str = 'fe2'\n",
    "dataset: str = 'LigBind'\n",
    "metrics_dir: str = os.path.join(root_dir, f'results_{dataset}_{target}')\n",
    "files: list = os.listdir(metrics_dir)\n",
    "with open(f'{root_dir}/totalMetrics_{target}.csv', 'w', encoding='utf-8',newline='') as writer_file: \n",
    "    writer = csv.writer(writer_file, delimiter=',')\n",
    "    writer.writerow(['target_ion', 'reduce_type', 'auc_roc','auc_pr','precision','recall','f1', 'mcc','accuracy'])\n",
    "    for file in files:\n",
    "        if file.startswith('type'):\n",
    "            metrics_path: str = os.path.join(metrics_dir, file)\n",
    "        assert os.path.isdir(metrics_path), f'error {metrics_path} is not dir'\n",
    "        metrics_path: str = os.path.join(metrics_path, 'metrics.txt')\n",
    "        with open(metrics_path, 'r', encoding='utf-8') as reader:\n",
    "            output = list(map(float, reader.readlines()[-1].split('\\t')))\n",
    "            output.insert(0, file)\n",
    "            output.insert(0, target)\n",
    "            writer.writerow(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93431262",
   "metadata": {},
   "source": [
    "* 数据统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = './noreduce/train'\n",
    "\n",
    "files = os.listdir(input_dir)\n",
    "for file in files:\n",
    "    positive_file = os.path.join(input_dir, file, 'positive_fragment.txt')\n",
    "    negative_file = os.path.join(input_dir, file, 'negative_fragment.txt')\n",
    "    positive_dict = []\n",
    "    negative_dict = []\n",
    "    with open(positive_file, 'r' , encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                positive_dict.append(line.strip())\n",
    "                # print(line.strip()[0:5])\n",
    "\n",
    "    with open(negative_file, 'r' , encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                negative_dict.append(line.strip())\n",
    "    positive_len = len(positive_dict)\n",
    "    negative_len = len(negative_dict)\n",
    "    total_len = positive_len+negative_len\n",
    "    reduce_len = total_len*671\n",
    "    print(\n",
    "        file, \n",
    "        '\\t', \n",
    "        f'positive:{positive_len}, negative:{negative_len}, total:{total_len}, reduce_len:{reduce_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6def32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 定义特征描述\n",
    "# 关键：这里必须和写入tfrecord文件时的特征结构完全一致\n",
    "feature_description = {\n",
    "    'protein_name': tf.io.FixedLenFeature([], tf.string),  # 字节特征\n",
    "    'center_res': tf.io.FixedLenFeature([], tf.string),   # 整型特征\n",
    "    'input_ids': tf.io.FixedLenFeature([27], tf.int64), # 浮点特征\n",
    "    'input_mask': tf.io.FixedLenFeature([27], tf.int64), # 浮点特征\n",
    "}\n",
    "\n",
    "# 2. 解析函数\n",
    "def parse_example(serialized_example):\n",
    "    parsed_features = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    # 如果需要，对解析后的特征进行后续处理，例如解码图像等\n",
    "    return parsed_features\n",
    "\n",
    "# 3. 读取并解析文件\n",
    "dataset = tf.data.TFRecordDataset('../temp.tfrecord')\n",
    "parsed_dataset = dataset.map(parse_example)\n",
    "\n",
    "# 4. 迭代查看内容\n",
    "for i, parsed_record in enumerate(parsed_dataset):\n",
    "    print(f\"Example {i}:\")\n",
    "    for feature_name, feature_value in parsed_record.items():\n",
    "        print(f\"  {feature_name}: {feature_value.numpy()}\") # 获取Tensor的numpy值\n",
    "    print()\n",
    "    if i >= 4:  # 限制打印前5个样本，避免输出过长\n",
    "        print(\"... (and more)\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15aeca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ysq/需备份文件/T5模型参数/data_make'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = os.getcwd()\n",
    "root_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
